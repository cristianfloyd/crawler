{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b7ee37",
   "metadata": {},
   "outputs": [],
   "source": [
    "!playwright install\n",
    "# Instalar dependencias primero\n",
    "# !pip install requests beautifulsoup4 html2text lxml pandas\n",
    "# !pip install crawl4ai nest-asyncio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5a30a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import html2text\n",
    "import pandas as pd\n",
    "from urllib.parse import urljoin, urlparse\n",
    "\n",
    "def scrape_website(url, css_selector=None):\n",
    "    \"\"\"\n",
    "    Scraper simple y confiable para Jupyter\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Headers para evitar bloqueos\n",
    "        headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
    "            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n",
    "            'Accept-Language': 'es-ES,es;q=0.5',\n",
    "            'Accept-Encoding': 'gzip, deflate',\n",
    "            'Connection': 'keep-alive',\n",
    "        }\n",
    "        \n",
    "        print(f\"üîç Scrapeando: {url}\")\n",
    "        response = requests.get(url, headers=headers, timeout=30)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        # Parsear HTML\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # Limpiar contenido no deseado\n",
    "        for element in soup(['script', 'style', 'nav', 'footer', 'header', 'aside']):\n",
    "            element.decompose()\n",
    "        \n",
    "        # Extraer contenido espec√≠fico si se proporciona selector\n",
    "        if css_selector:\n",
    "            selected_elements = soup.select(css_selector)\n",
    "            if selected_elements:\n",
    "                content_soup = BeautifulSoup('', 'html.parser')\n",
    "                for elem in selected_elements:\n",
    "                    content_soup.append(elem)\n",
    "                soup = content_soup\n",
    "        \n",
    "        # Convertir a markdown\n",
    "        h = html2text.HTML2Text()\n",
    "        h.ignore_links = False\n",
    "        h.ignore_images = False\n",
    "        h.body_width = 0  # No limitar ancho\n",
    "        markdown_content = h.handle(str(soup))\n",
    "        \n",
    "        # Extraer texto plano\n",
    "        text_content = soup.get_text(strip=True, separator=' ')\n",
    "        \n",
    "        return {\n",
    "            'url': url,\n",
    "            'title': soup.title.string if soup.title else 'Sin t√≠tulo',\n",
    "            'markdown': markdown_content,\n",
    "            'text': text_content,\n",
    "            'html': response.text,\n",
    "            'status_code': response.status_code,\n",
    "            'success': True\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        return {\n",
    "            'url': url,\n",
    "            'error': str(e),\n",
    "            'success': False\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "546bc172",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Scrapeando: https://exactas.uba.ar/ensenanza/carreras-de-grado/ciencias-de-datos/\n",
      "‚úÖ Scraping exitoso!\n",
      "üìÑ T√≠tulo: Ciencias de Datos | Facultad de Ciencias Exactas y Naturales de la Universidad de Buenos Aires\n",
      "üìä Longitud del contenido: 12598 caracteres\n",
      "\n",
      "üìù Primeros 1000 caracteres del markdown:\n",
      "--------------------------------------------------\n",
      "  * [![twitter](https://exactas.uba.ar/wp-content/themes/polytechnic/mythology-core/core-assets/images/social-icons/circles/twitter.png)](https://twitter.com/exactas_uba)\n",
      "  * [![facebook](https://exactas.uba.ar/wp-content/themes/polytechnic/mythology-core/core-assets/images/social-icons/circles/fb.png)](https://www.facebook.com/UBAExactas)\n",
      "  * [![youtube](https://exactas.uba.ar/wp-content/themes/polytechnic/mythology-core/core-assets/images/social-icons/circles/youtube.png)](https://www.youtube.com/exactasubaoficial)\n",
      "  * [![linkedin](https://exactas.uba.ar/wp-content/themes/polytechnic/mythology-core/core-assets/images/social-icons/circles/linkedin.png)](https://www.linkedin.com/company/facultad-de-ciencias-exactas-y-naturales-uba)\n",
      "  * [![instagram](https://exactas.uba.ar/wp-content/themes/polytechnic/mythology-core/core-assets/images/social-icons/circles/instagram.png)](https://www.instagram.com/exactas_uba/)\n",
      "\n",
      "\n",
      "\n",
      "# [ ![Facultad de Ciencias Exactas y Naturales de la Universidad de Bueno\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Probar con la URL de exactas\n",
    "url = \"https://exactas.uba.ar/ensenanza/carreras-de-grado/ciencias-de-datos/\"\n",
    "result = scrape_website(url)\n",
    "\n",
    "if result['success']:\n",
    "    print(\"‚úÖ Scraping exitoso!\")\n",
    "    print(f\"üìÑ T√≠tulo: {result['title']}\")\n",
    "    print(f\"üìä Longitud del contenido: {len(result['markdown'])} caracteres\")\n",
    "    print(\"\\nüìù Primeros 1000 caracteres del markdown:\")\n",
    "    print(\"-\" * 50)\n",
    "    print(result['markdown'][:1000])\n",
    "    print(\"-\" * 50)\n",
    "else:\n",
    "    print(f\"‚ùå Error: {result['error']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b7a0151",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "import asyncio\n",
    "from crawl4ai import AsyncWebCrawler\n",
    "# Habilitar loops anidados para Jupyter\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "10788251",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def crawl_with_crawl4ai(url):\n",
    "    \"\"\"\n",
    "    Usar Crawl4AI en Jupyter con configuraci√≥n espec√≠fica\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Configuraci√≥n optimizada para Jupyter\n",
    "        config = {\n",
    "            'headless': True,\n",
    "            'verbose': False,\n",
    "            'browser_type': 'chromium',\n",
    "            'ignore_https_errors': True,\n",
    "            'java_script_enabled': False,  # Deshabilitar JS para mayor compatibilidad\n",
    "        }\n",
    "        \n",
    "        print(f\"üöÄ Iniciando crawler para: {url}\")\n",
    "        \n",
    "        async with AsyncWebCrawler(**config) as crawler:\n",
    "            result = await crawler.arun(\n",
    "                url=url,\n",
    "                bypass_cache=True,\n",
    "                process_iframes=False,\n",
    "                wait_for_images=False,\n",
    "                simulate_user=False,\n",
    "                magic=True,  # Usar extracci√≥n inteligente\n",
    "            )\n",
    "            \n",
    "            print(f\"‚úÖ Crawling completado!\")\n",
    "            print(f\"üìã Tipo de resultado: {type(result)}\")\n",
    "            \n",
    "            # Mostrar atributos disponibles\n",
    "            attrs = [attr for attr in dir(result) if not attr.startswith('_')]\n",
    "            print(f\"üìã Atributos disponibles: {attrs}\")\n",
    "            \n",
    "            return result\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error en Crawl4AI: {e}\")\n",
    "        return None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9bd5cdd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Iniciando crawler para: https://exactas.uba.ar/ensenanza/carreras-de-grado/ciencias-de-datos/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-2' coro=<Connection.run() done, defined at c:\\Users\\arca\\Desarrollo\\Exacty\\.venv\\Lib\\site-packages\\playwright\\_impl\\_connection.py:303> exception=NotImplementedError()>\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\laragon\\bin\\python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\arca\\Desarrollo\\Exacty\\.venv\\Lib\\site-packages\\playwright\\_impl\\_connection.py\", line 310, in run\n",
      "    await self._transport.connect()\n",
      "  File \"c:\\Users\\arca\\Desarrollo\\Exacty\\.venv\\Lib\\site-packages\\playwright\\_impl\\_transport.py\", line 133, in connect\n",
      "    raise exc\n",
      "  File \"c:\\Users\\arca\\Desarrollo\\Exacty\\.venv\\Lib\\site-packages\\playwright\\_impl\\_transport.py\", line 120, in connect\n",
      "    self._proc = await asyncio.create_subprocess_exec(\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\laragon\\bin\\python\\Python312\\Lib\\asyncio\\subprocess.py\", line 224, in create_subprocess_exec\n",
      "    transport, protocol = await loop.subprocess_exec(\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\laragon\\bin\\python\\Python312\\Lib\\asyncio\\base_events.py\", line 1756, in subprocess_exec\n",
      "    transport = await self._make_subprocess_transport(\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\laragon\\bin\\python\\Python312\\Lib\\asyncio\\base_events.py\", line 528, in _make_subprocess_transport\n",
      "    raise NotImplementedError\n",
      "NotImplementedError\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Error en Crawl4AI: \n"
     ]
    }
   ],
   "source": [
    "# Ejecutar el crawler\n",
    "url = \"https://exactas.uba.ar/ensenanza/carreras-de-grado/ciencias-de-datos/\"\n",
    "result = asyncio.run(crawl_with_crawl4ai(url))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e063bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "if result:\n",
    "    # Verificar qu√© atributos tiene el resultado\n",
    "    if hasattr(result, 'markdown') and result.markdown:\n",
    "        print(\"\\nüìÑ Contenido Markdown:\")\n",
    "        print(\"-\" * 50)\n",
    "        print(result.markdown[:1000])\n",
    "    elif hasattr(result, 'extracted_content') and result.extracted_content:\n",
    "        print(\"\\nüìÑ Contenido Extra√≠do:\")\n",
    "        print(\"-\" * 50)\n",
    "        print(result.extracted_content[:1000])\n",
    "    else:\n",
    "        print(\"\\nüîç Explorando resultado:\")\n",
    "        for attr in ['html', 'text', 'content']:\n",
    "            if hasattr(result, attr):\n",
    "                content = getattr(result, attr)\n",
    "                if content:\n",
    "                    print(f\"‚úÖ Encontrado {attr}: {str(content)[:200]}...\")\n",
    "                    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
