# Plan de Acci√≥n: Sistema de scraping para RAG

## üìã Resumen del Proyecto

**Objetivo**: Construir un sistema RAG (Retrieval Augmented Generation) completo para crear un chatbot que responda preguntas sobre materias universitarias del sitio web https://lcd.exactas.uba.ar/materias/

**Fuente de datos**: Aplicaci√≥n WordPress con informaci√≥n de materias de la Facultad de Ciencias Exactas, UBA

**Tecnolog√≠as principales**: Python, Crawl4AI, SentenceTransformers, FAISS, BeautifulSoup

---

## üéØ Estado Actual

- ‚úÖ **Scraper funcional**: Ya tenemos `scrap_materias_rag_corregido.py` funcionando con CosineStrategy
- ‚è≥ **Pendiente**: Pipeline completo de RAG y sistema de consultas

---

## üìä Fases del Proyecto

### **FASE 1: EXTRACCI√ìN Y PREPARACI√ìN DE DATOS** 

#### 1.0 Descubrimiento y Mapeo de Sitios üó∫Ô∏è NUEVO
- [X] **Archivo**: `descubrir_sitios.py`
- [ ] **Tareas**:
  - [ ] Scraping de p√°gina principal https://lcd.exactas.uba.ar/ para mapear estructura
  - [ ] Identificar todos los departamentos/institutos (Matem√°tica, Computaci√≥n, F√≠sica, etc.)
  - [ ] Descubrir subdominios y sitios relacionados de cada departamento
  - [ ] Analizar tecnolog√≠as diferentes por sitio (WordPress, Drupal, HTML est√°tico, etc.)
  - [ ] Crear inventario de URLs por categor√≠a (materias, horarios, docentes, etc.)
  - [ ] Detectar patrones de estructura por sitio
  - [ ] Priorizar sitios por relevancia y viabilidad t√©cnica
- [ ] **Entregables**: 
  - [ ] `inventario_sitios.json` con mapeo completo
  - [ ] `estrategias_por_sitio.json` con approach t√©cnico por sitio
  - [ ] Reporte de factibilidad t√©cnica
- [ ] **Criterios de √©xito**: 
  - [ ] Mapeo de 100% de departamentos principales
  - [ ] Identificaci√≥n de al menos 80% de fuentes de informaci√≥n de materias
  - [ ] Estrategia t√©cnica definida para cada tipo de sitio

#### 1.1 Scrapers Especializados por Tecnolog√≠a üîß AMPLIADO
- [ ] **Archivos**: `scrapers/` (directorio)
  - [ ] `scraper_wordpress.py` (base actual, mejorado)
  - [ ] `scraper_drupal.py` (para sitios Drupal)
  - [ ] `scraper_joomla.py` (para sitios Joomla)
  - [ ] `scraper_html_estatico.py` (para sitios simples)
  - [ ] `scraper_pdfs.py` (para documentos PDF)
  - [ ] `scraper_base.py` (clase abstracta com√∫n)
- [ ] **Tareas**:
  - [ ] Refactorizar scraper actual como especialista en WordPress
  - [ ] Desarrollar scrapers espec√≠ficos por tecnolog√≠a detectada
  - [ ] Implementar detecci√≥n autom√°tica de tecnolog√≠a por sitio
  - [ ] Sistema de fallback entre estrategias de scraping
  - [ ] Manejo de rate limiting y robots.txt por sitio
  - [ ] Extracci√≥n de metadatos espec√≠ficos (departamento, tipo de informaci√≥n)
  - [ ] Sistema de validaci√≥n de contenido extra√≠do
- [ ] **Entregables**: 
  - [ ] Suite de scrapers especializados
  - [ ] `datos_por_sitio/` con datos organizados por fuente
  - [ ] Logs detallados de √©xito/fallo por sitio
- [ ] **Criterios de √©xito**: 
  - [ ] 95% de sitios scrapeados exitosamente
  - [ ] Contenido extra√≠do con calidad consistente
  - [ ] Tiempo total de scraping < 2 horas

#### 1.2 Coordinador de Scraping Masivo üï∑Ô∏è NUEVO
- [ ] **Archivo**: `scraper_coordinador.py`
- [ ] **Tareas**:
  - [ ] Orquestaci√≥n de scraping en paralelo por sitio
  - [ ] Sistema de prioridades por importancia del sitio
  - [ ] Monitoreo de progreso en tiempo real
  - [ ] Manejo de fallos y reintentos inteligentes
  - [ ] Respeto de l√≠mites de velocidad por dominio
  - [ ] Consolidaci√≥n de datos de m√∫ltiples fuentes
  - [ ] Detecci√≥n de contenido duplicado entre sitios
- [ ] **Entregables**: 
  - [ ] Sistema de scraping coordinado
  - [ ] Dashboard de progreso
  - [ ] `materias_consolidadas.json` con datos unificados
- [ ] **Criterios de √©xito**: 
  - [ ] Scraping sin bloqueos por rate limiting
  - [ ] Consolidaci√≥n exitosa de datos multi-fuente
  - [ ] Recuperaci√≥n autom√°tica de errores menores

#### 1.3 Limpieza y Estructuraci√≥n de Datos Multi-fuente üìù AMPLIADO
- [ ] **Archivo**: `preparar_datos_rag.py` (expandido)
- [ ] **Tareas**:
  - [ ] Normalizaci√≥n de datos de fuentes heterog√©neas
  - [ ] Mapeo de esquemas diferentes a estructura com√∫n
  - [ ] Detecci√≥n y resoluci√≥n de duplicados entre sitios
  - [ ] Enriquecimiento con informaci√≥n cruzada (ej: horarios + programas)
  - [ ] Extracci√≥n de metadatos espec√≠ficos por tipo de fuente
  - [ ] Validaci√≥n de consistencia entre fuentes
  - [ ] Generaci√≥n de jerarqu√≠a: Facultad > Departamento > Materia > Informaci√≥n
- [ ] **Entregables**: 
  - [ ] `documentos_rag_multifuente.json` con datos unificados
  - [ ] `esquema_unificado.json` con estructura com√∫n
  - [ ] Reporte de calidad y cobertura por fuente
- [ ] **Criterios de √©xito**:
  - [ ] 100% compatibilidad entre esquemas de fuentes
  - [ ] < 3% de informaci√≥n duplicada
  - [ ] Cobertura balanceada entre departamentos

#### 1.4 Validaci√≥n y Control de Calidad Multi-sitio üîç AMPLIADO  
- [ ] **Archivo**: `validar_datos_multisitio.py`
- [ ] **Tareas**:
  - [ ] Estad√≠sticas por fuente y consolidadas
  - [ ] An√°lisis de cobertura por departamento/materia
  - [ ] Detecci√≥n de informaci√≥n faltante cr√≠tica
  - [ ] Validaci√≥n cruzada entre fuentes
  - [ ] Identificaci√≥n de inconsistencias entre sitios
  - [ ] An√°lisis de freshness de informaci√≥n por fuente
  - [ ] M√©tricas de calidad por tipo de scraper
- [ ] **Entregables**:
  - [ ] `reporte_calidad_multisitio.html` con an√°lisis por fuente
  - [ ] `matriz_cobertura.json` con gaps identificados
  - [ ] Priorizaci√≥n de sitios para re-scraping
- [ ] **Criterios de √©xito**:
  - [ ] Cobertura > 80% para cada departamento principal
  - [ ] Identificaci√≥n de 100% de fuentes cr√≠ticas faltantes
  - [ ] Plan de acci√≥n para gaps importantes

---

### **FASE 2: SISTEMA DE EMBEDDINGS Y B√öSQUEDA**

#### 2.1 Configuraci√≥n del Sistema de Embeddings ü§ñ NUEVO
- [ ] **Archivo**: `sistema_embeddings.py`
- [ ] **Tareas**:
  - [ ] Evaluar diferentes modelos de embeddings en espa√±ol
  - [ ] Benchmark de velocidad y calidad: `paraphrase-multilingual-MiniLM-L12-v2`, `intfloat/multilingual-e5-base`
  - [ ] Configurar par√°metros √≥ptimos de embedding
  - [ ] Implementar procesamiento en lotes para eficiencia
  - [ ] A√±adir cache de embeddings para desarrollo
  - [ ] Manejar textos muy largos (chunking autom√°tico)
- [ ] **Entregables**:
  - [ ] `benchmark_modelos.json` con m√©tricas de comparaci√≥n
  - [ ] Sistema de embeddings optimizado
- [ ] **Criterios de √©xito**:
  - [ ] Embeddings generados en < 10 minutos
  - [ ] Similitud sem√°ntica > 0.8 en casos de prueba
  - [ ] Uso de memoria < 4GB

#### 2.2 √çndice Vectorial con FAISS üìä NUEVO
- [ ] **Archivo**: `sistema_embeddings.py` (expandido)
- [ ] **Tareas**:
  - [ ] Comparar tipos de √≠ndices FAISS (Flat, IVF, HNSW)
  - [ ] Optimizar par√°metros de b√∫squeda
  - [ ] Implementar b√∫squeda h√≠brida (vectorial + filtros)
  - [ ] A√±adir filtrado por metadatos (materia, tipo, etc.)
  - [ ] Implementar re-ranking por relevancia
  - [ ] Optimizar para consultas en tiempo real
- [ ] **Entregables**:
  - [ ] `rag_sistema/` con √≠ndice optimizado
  - [ ] Benchmarks de velocidad de b√∫squeda
- [ ] **Criterios de √©xito**:
  - [ ] B√∫squeda < 100ms para consultas t√≠picas
  - [ ] Precision@5 > 80% en casos de prueba
  - [ ] Recall@10 > 90% en casos de prueba

#### 2.3 Sistema de Evaluaci√≥n y M√©tricas üìà NUEVO
- [ ] **Archivo**: `evaluar_rag.py`
- [ ] **Tareas**:
  - [ ] Crear dataset de pruebas con consultas y respuestas esperadas
  - [ ] Implementar m√©tricas: Precision, Recall, NDCG, MRR
  - [ ] Evaluaci√≥n de relevancia sem√°ntica
  - [ ] A/B testing entre configuraciones
  - [ ] An√°lisis de casos de fallo
  - [ ] M√©tricas de cobertura por materia
- [ ] **Entregables**:
  - [ ] `dataset_evaluacion.json` con casos de prueba
  - [ ] `reporte_metricas.html` con resultados
  - [ ] Sistema de evaluaci√≥n automatizada
- [ ] **Criterios de √©xito**:
  - [ ] > 100 casos de prueba diversos
  - [ ] Precision@5 > 85%
  - [ ] Cobertura balanceada entre materias

---

### **FASE 3: PIPELINE Y AUTOMATIZACI√ìN**

#### 3.1 Pipeline de Procesamiento Completo üîÑ MEJORAR
- [ ] **Archivo**: `pipeline_rag_completo.py`
- [ ] **Tareas**:
  - [ ] Integrar todos los pasos en pipeline √∫nico
  - [ ] A√±adir checkpoints para reanudar procesamiento
  - [ ] Implementar rollback en caso de errores
  - [ ] Logging comprehensivo de todo el proceso
  - [ ] Monitoreo de recursos (CPU, memoria, disco)
  - [ ] Notificaciones de √©xito/fallo
  - [ ] Paralelizaci√≥n donde sea posible
- [ ] **Entregables**:
  - [ ] Pipeline robusto end-to-end
  - [ ] Documentaci√≥n de troubleshooting
- [ ] **Criterios de √©xito**:
  - [ ] Ejecuci√≥n exitosa sin supervisi√≥n
  - [ ] Tiempo total < 30 minutos
  - [ ] Recovery autom√°tico de errores menores

#### 3.2 Sistema de Consultas y API üîç MEJORAR
- [ ] **Archivo**: `consultar_rag.py`
- [ ] **Tareas**:
  - [ ] Mejorar interfaz de l√≠nea de comandos
  - [ ] A√±adir filtros avanzados (por materia, tipo, fecha)
  - [ ] Implementar b√∫squeda con operadores (AND, OR, NOT)
  - [ ] Sistema de autocompletado para materias
  - [ ] Exportar resultados (JSON, CSV, markdown)
  - [ ] Historial de consultas
  - [ ] M√©tricas de uso en tiempo real
- [ ] **Entregables**:
  - [ ] Interfaz CLI completa
  - [ ] API REST b√°sica (Flask/FastAPI)
- [ ] **Criterios de √©xito**:
  - [ ] Consultas complejas ejecutadas correctamente
  - [ ] Respuestas en < 200ms
  - [ ] Interfaz intuitiva para usuarios no t√©cnicos

#### 3.3 Actualizaci√≥n y Mantenimiento üîÑ NUEVO
- [ ] **Archivo**: `actualizar_rag.py`
- [ ] **Tareas**:
  - [ ] Sistema de detecci√≥n de cambios en el sitio web
  - [ ] Scraping incremental (solo nuevos contenidos)
  - [ ] Versionado del sistema RAG
  - [ ] Backup y restore de √≠ndices
  - [ ] Limpieza autom√°tica de datos obsoletos
  - [ ] Alertas de degradaci√≥n de calidad
- [ ] **Entregables**:
  - [ ] Sistema de actualizaci√≥n autom√°tica
  - [ ] Procedimientos de mantenimiento
- [ ] **Criterios de √©xito**:
  - [ ] Detecci√≥n de cambios en < 24 horas
  - [ ] Actualizaci√≥n incremental sin downtime
  - [ ] Preservaci√≥n de calidad tras actualizaciones

---

### **FASE 4: DOCUMENTACI√ìN Y TESTING**

#### 4.1 Testing Comprehensivo üß™ NUEVO
- [ ] **Archivo**: `tests/`
- [ ] **Tareas**:
  - [ ] Unit tests para cada m√≥dulo
  - [ ] Integration tests para pipeline completo
  - [ ] Performance tests con datasets grandes
  - [ ] Stress tests de consultas concurrentes
  - [ ] Tests de regresi√≥n
  - [ ] Tests de edge cases (consultas vac√≠as, muy largas, etc.)
- [ ] **Entregables**:
  - [ ] Suite de tests automatizada
  - [ ] CI/CD pipeline b√°sico
- [ ] **Criterios de √©xito**:
  - [ ] 90%+ cobertura de c√≥digo
  - [ ] Todos los tests pasan consistentemente
  - [ ] Tests ejecutados en < 5 minutos

#### 4.2 Documentaci√≥n T√©cnica üìö NUEVO
- [ ] **Archivos**: `docs/`
- [ ] **Tareas**:
  - [ ] README.md con instrucciones de instalaci√≥n
  - [ ] Gu√≠a de usuario detallada
  - [ ] Documentaci√≥n de API
  - [ ] Troubleshooting guide
  - [ ] Arquitectura del sistema
  - [ ] Best practices y optimizaciones
  - [ ] Ejemplos de uso com√∫n
- [ ] **Entregables**:
  - [ ] Documentaci√≥n completa en markdown
  - [ ] Diagramas de arquitectura
- [ ] **Criterios de √©xito**:
  - [ ] Usuario nuevo puede instalar y usar sin ayuda
  - [ ] Documentaci√≥n actualizada en cada release

#### 4.3 Optimizaci√≥n Final y Deployment üöÄ NUEVO
- [ ] **Tareas**:
  - [ ] Profiling de performance completo
  - [ ] Optimizaci√≥n de memoria y CPU
  - [ ] Configuraci√≥n para diferentes entornos (dev/prod)
  - [ ] Dockerizaci√≥n del sistema
  - [ ] Scripts de deployment
  - [ ] Monitoreo y alertas b√°sicas
- [ ] **Entregables**:
  - [ ] Sistema optimizado y production-ready
  - [ ] Gu√≠a de deployment
- [ ] **Criterios de √©xito**:
  - [ ] Sistema puede manejar 100+ consultas/minuto
  - [ ] Deployment automatizado funcionando
  - [ ] Downtime < 1 minuto para actualizaciones

---

## üóÇÔ∏è Estructura de Archivos Esperada

```
proyecto_rag_materias/
‚îú‚îÄ‚îÄ README.md
‚îú‚îÄ‚îÄ requirements.txt
‚îú‚îÄ‚îÄ PLAN_RAG_MATERIAS.md
‚îÇ
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ scrap_materias_rag_corregido.py
‚îÇ   ‚îú‚îÄ‚îÄ preparar_datos_rag.py
‚îÇ   ‚îú‚îÄ‚îÄ sistema_embeddings.py
‚îÇ   ‚îú‚îÄ‚îÄ pipeline_rag_completo.py
‚îÇ   ‚îú‚îÄ‚îÄ consultar_rag.py
‚îÇ   ‚îú‚îÄ‚îÄ actualizar_rag.py
‚îÇ   ‚îú‚îÄ‚îÄ evaluar_rag.py
‚îÇ   ‚îî‚îÄ‚îÄ validar_datos.py
‚îÇ
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îú‚îÄ‚îÄ materias_rag.json
‚îÇ   ‚îú‚îÄ‚îÄ documentos_rag_final.json
‚îÇ   ‚îú‚îÄ‚îÄ esquema_metadatos.json
‚îÇ   ‚îî‚îÄ‚îÄ dataset_evaluacion.json
‚îÇ
‚îú‚îÄ‚îÄ rag_sistema/
‚îÇ   ‚îú‚îÄ‚îÄ indice.faiss
‚îÇ   ‚îú‚îÄ‚îÄ documentos.json
‚îÇ   ‚îú‚îÄ‚îÄ metadatos.json
‚îÇ   ‚îî‚îÄ‚îÄ config.json
‚îÇ
‚îú‚îÄ‚îÄ tests/
‚îÇ   ‚îú‚îÄ‚îÄ test_scraper.py
‚îÇ   ‚îú‚îÄ‚îÄ test_embeddings.py
‚îÇ   ‚îú‚îÄ‚îÄ test_pipeline.py
‚îÇ   ‚îî‚îÄ‚îÄ test_consultas.py
‚îÇ
‚îú‚îÄ‚îÄ docs/
‚îÇ   ‚îú‚îÄ‚îÄ arquitectura.md
‚îÇ   ‚îú‚îÄ‚îÄ guia_usuario.md
‚îÇ   ‚îú‚îÄ‚îÄ api_reference.md
‚îÇ   ‚îî‚îÄ‚îÄ troubleshooting.md
‚îÇ
‚îú‚îÄ‚îÄ scripts/
‚îÇ   ‚îú‚îÄ‚îÄ install.sh
‚îÇ   ‚îú‚îÄ‚îÄ deploy.sh
‚îÇ   ‚îî‚îÄ‚îÄ backup.sh
‚îÇ
‚îî‚îÄ‚îÄ reports/
    ‚îú‚îÄ‚îÄ reporte_calidad_datos.html
    ‚îú‚îÄ‚îÄ benchmark_modelos.json
    ‚îî‚îÄ‚îÄ reporte_metricas.html
```

---

## ‚è±Ô∏è Cronograma Estimado

| Fase | Duraci√≥n | Prioridad | Dependencias |
|------|----------|-----------|--------------|
| **Fase 1**: Datos | 3-4 d√≠as | Alta | - |
| **Fase 2**: Embeddings | 2-3 d√≠as | Alta | Fase 1 |
| **Fase 3**: Pipeline | 2-3 d√≠as | Media | Fase 2 |
| **Fase 4**: Testing/Docs | 2-3 d√≠as | Media | Fase 3 |
| **TOTAL** | **9-13 d√≠as** | | |

---

## üéØ M√©tricas de √âxito del Proyecto

### M√©tricas T√©cnicas
- [ ] **Calidad de Datos**: < 5% documentos con errores
- [ ] **Performance**: Consultas < 200ms
- [ ] **Precisi√≥n**: Precision@5 > 85%
- [ ] **Cobertura**: 100% materias scrapeadas
- [ ] **Confiabilidad**: 99% uptime del sistema

### M√©tricas de Usabilidad  
- [ ] **Facilidad de uso**: Usuario nuevo puede usar en < 10 minutos
- [ ] **Documentaci√≥n**: 100% funcionalidades documentadas
- [ ] **Mantenibilidad**: Actualizaciones autom√°ticas funcionando

---

## üö® Riesgos Identificados

| Riesgo | Probabilidad | Impacto | Mitigaci√≥n |
|--------|--------------|---------|------------|
| Cambios en estructura del sitio web | Media | Alto | Scraper robusto + monitoreo |
| Modelos de embedding pesados | Baja | Medio | Benchmark m√∫ltiples modelos |
| Calidad irregular de datos | Media | Medio | Validaci√≥n autom√°tica exhaustiva |
| Performance en consultas complejas | Media | Medio | Optimizaci√≥n FAISS + cache |

---

## üìù Notas y Consideraciones

### **Decisiones T√©cnicas Pendientes - ACTUALIZADAS**
- [ ] **Estrategia de scraping**: ¬øScraping en paralelo o secuencial por prioridad?
- [ ] **Gesti√≥n de datos**: ¬øBase de datos (SQLite/PostgreSQL) o solo archivos JSON?
- [ ] **Modelo de embedding**: ¬øMultilingual o espec√≠fico espa√±ol?
- [ ] **Tipo de √≠ndice FAISS**: ¬øFlat, IVF o HNSW para datasets grandes?
- [ ] **Chunking strategy**: ¬øCosineStrategy vs manual vs h√≠brido?
- [ ] **Actualizaci√≥n**: ¬øSistema de detecci√≥n de cambios por sitio?
- [ ] **Rate limiting**: ¬øPol√≠tica agresiva vs conservadora por dominio?

### Escalabilidad Futura
- [ ] Soporte para m√∫ltiples universidades
- [ ] Integraci√≥n con sistemas de gesti√≥n acad√©mica
- [ ] API p√∫blica para desarrolladores
- [ ] Interface web interactiva
- [ ] Chatbot con LLM integrado

---

## ‚úÖ Checklist de Progreso

**Completado:**
- [x] Scraper b√°sico funcionando
- [x] CosineStrategy implementado
- [x] Plan de acci√≥n definido

**En Progreso:**
- [ ] Optimizaci√≥n del scraper
- [ ] Pipeline de preparaci√≥n de datos

**Pendiente:**
- [ ] Sistema de embeddings
- [ ] Validaci√≥n y testing
- [ ] Documentaci√≥n completa