#!/usr/bin/env python3
"""
Script de prueba simplificado para diagnosticar problemas con LLM Studio
"""

import asyncio
import os
import requests
from crawl4ai import AsyncWebCrawler, LLMConfig
from crawl4ai.extraction_strategy import LLMExtractionStrategy
from dotenv import load_dotenv

# Cargar variables de entorno
load_dotenv()

async def test_llm_studio_connection():
    """Prueba b√°sica de conexi√≥n con LLM Studio"""
    print("üß™ PRUEBA: Conexi√≥n b√°sica con LLM Studio")
    print("="*50)
    
    # Verificar configuraci√≥n
    llm_studio_url = os.getenv("LLM_STUDIO_BASE_URL")
    llm_studio_model = os.getenv("LLM_STUDIO_MODEL", "gemma-3")
    
    if not llm_studio_url:
        print("‚ùå Error: LLM_STUDIO_BASE_URL no configurado en .env")
        return False
    
    print(f"‚úÖ Configuraci√≥n detectada:")
    print(f"   URL: {llm_studio_url}")
    print(f"   Modelo: {llm_studio_model}")
    
    # Crear configuraci√≥n LLM
    llm_config = LLMConfig(
        provider=f"openai/{llm_studio_model}",
        base_url=llm_studio_url,
        api_token="not-needed"  # LLM Studio local no requiere token real
    )
    
    print(f"üîß Configuraci√≥n LLM creada:")
    print(f"   Provider: {llm_config.provider}")
    print(f"   Base URL: {llm_config.base_url}")
    print(f"   API Token: {llm_config.api_token}")
    
    try:
        async with AsyncWebCrawler(
            verbose=True,  # Habilitar verbose para ver m√°s detalles
            headless=True,
            always_by_pass_cache=True,
            browser_type="chromium"
        ) as crawler:
            
            print("\nüì° Probando extracci√≥n simple...")
            
            result = await crawler.arun(
                url="https://lcd.exactas.uba.ar/materias",
                extraction_strategy=LLMExtractionStrategy(
                    llm_config=llm_config,
                    extraction_type="llm",
                    instruction="What is this webpage about? Answer in one sentence.",
                    chunk_token_threshold=1000,
                    apply_chunking=False  # Deshabilitar chunking para prueba simple
                ),
                bypass_cache=True
            )
            
            print(f"\nüìä Resultado de la prueba:")
            print(f"   Success: {result.success if result else 'No result'}")
            print(f"   Has HTML: {bool(result and result.html)}")
            print(f"   HTML Length: {len(result.html) if result and result.html else 0}")
            print(f"   Has Markdown: {bool(result and result.markdown)}")
            print(f"   Markdown Length: {len(result.markdown) if result and result.markdown else 0}")
            print(f"   Has Extracted Content: {bool(result and hasattr(result, 'extracted_content') and result.extracted_content)}")
            
            if result and hasattr(result, 'extracted_content'):
                print(f"   Extracted Content: '{result.extracted_content}'")
            else:
                print(f"   Extracted Content: None")
            
            if result and hasattr(result, 'error_message') and result.error_message:
                print(f"   Error Message: {result.error_message}")
            
            # Verificar si el problema es espec√≠fico del LLM
            if result and result.success and result.html and not result.extracted_content:
                print(f"\nüîç DIAGN√ìSTICO: El crawler funciona pero el LLM no responde")
                print(f"   - HTML obtenido correctamente")
                print(f"   - LLM Studio no est√° procesando la solicitud")
                print(f"   - Posibles causas:")
                print(f"     1. LLM Studio no est√° corriendo en {llm_studio_url}")
                print(f"     2. El modelo {llm_studio_model} no est√° cargado")
                print(f"     3. Problema de conectividad de red")
                print(f"     4. Configuraci√≥n incorrecta del endpoint")
                return False
            elif result and result.extracted_content:
                print(f"\n‚úÖ √âXITO: LLM Studio responde correctamente")
                return True
            else:
                print(f"\n‚ùå FALLO: Problema general con el crawler o LLM")
                return False
                
    except Exception as e:
        print(f"\n‚ùå Error durante la prueba: {e}")
        return False

def test_network_connectivity():
    """Prueba conectividad de red b√°sica"""
    print("\nüåê PRUEBA: Conectividad de red")
    print("="*50)
    
    llm_studio_url = os.getenv("LLM_STUDIO_BASE_URL")
    if not llm_studio_url:
        print("‚ùå LLM_STUDIO_BASE_URL no configurado")
        return False
    
    try:
        # Probar conectividad b√°sica
        response = requests.get(f"{llm_studio_url}/v1/models", timeout=5)
        print(f"‚úÖ Conectividad exitosa: {response.status_code}")
        print(f"   Respuesta: {response.text[:200]}...")
        return True
    except requests.exceptions.ConnectionError:
        print(f"‚ùå Error de conexi√≥n: No se puede conectar a {llm_studio_url}")
        print("   Verifica que LLM Studio est√© corriendo")
        return False
    except requests.exceptions.Timeout:
        print(f"‚ùå Timeout: La conexi√≥n a {llm_studio_url} tard√≥ demasiado")
        return False
    except Exception as e:
        print(f"‚ùå Error inesperado: {e}")
        return False

async def test_alternative_configs():
    """Prueba configuraciones alternativas"""
    print("\nüîß PRUEBA: Configuraciones alternativas")
    print("="*50)
    
    llm_studio_url = os.getenv("LLM_STUDIO_BASE_URL")
    llm_studio_model = os.getenv("LLM_STUDIO_MODEL", "gemma-3")
    
    # Configuraci√≥n 1: Sin base_url (usar default)
    print("üìã Configuraci√≥n 1: Sin base_url")
    try:
        llm_config1 = LLMConfig(
            provider=f"openai/{llm_studio_model}",
            api_token="not-needed"
        )
        print(f"   ‚úÖ Configuraci√≥n 1 creada: {llm_config1.provider}")
    except Exception as e:
        print(f"   ‚ùå Error en configuraci√≥n 1: {e}")
    
    # Configuraci√≥n 2: Con base_url expl√≠cito
    print("üìã Configuraci√≥n 2: Con base_url expl√≠cito")
    try:
        llm_config2 = LLMConfig(
            provider=f"openai/{llm_studio_model}",
            base_url=llm_studio_url,
            api_token="not-needed"
        )
        print(f"   ‚úÖ Configuraci√≥n 2 creada: {llm_config2.provider} -> {llm_config2.base_url}")
    except Exception as e:
        print(f"   ‚ùå Error en configuraci√≥n 2: {e}")
    
    # Configuraci√≥n 3: Sin api_token
    print("üìã Configuraci√≥n 3: Sin api_token")
    try:
        llm_config3 = LLMConfig(
            provider=f"openai/{llm_studio_model}",
            base_url=llm_studio_url
        )
        print(f"   ‚úÖ Configuraci√≥n 3 creada: {llm_config3.provider}")
    except Exception as e:
        print(f"   ‚ùå Error en configuraci√≥n 3: {e}")

if __name__ == "__main__":
    print("üöÄ Iniciando diagn√≥stico de LLM Studio...")
    
    # Verificar archivo .env
    if not os.path.exists('.env'):
        print("‚ùå Error: Archivo .env no encontrado")
        print("   Crea un archivo .env con:")
        print("   LLM_STUDIO_BASE_URL=http://192.168.1.35:1234")
        print("   LLM_STUDIO_MODEL=gemma-3")
        exit(1)
    
    # Ejecutar pruebas
    print("\n" + "="*60)
    print("üîç DIAGN√ìSTICO COMPLETO DE LLM STUDIO")
    print("="*60)
    
    # 1. Prueba de conectividad de red
    network_ok = test_network_connectivity()
    
    # 2. Prueba configuraciones alternativas
    asyncio.run(test_alternative_configs())
    
    # 3. Prueba de conexi√≥n LLM (solo si la red funciona)
    if network_ok:
        success = asyncio.run(test_llm_studio_connection())
    else:
        success = False
    
    # Resumen final
    print("\n" + "="*60)
    print("üìã RESUMEN DEL DIAGN√ìSTICO")
    print("="*60)
    
    if success:
        print("üéâ LLM Studio funciona correctamente!")
    else:
        print("‚ùå LLM Studio tiene problemas de configuraci√≥n")
        print("\nüîß PASOS PARA SOLUCIONAR:")
        print("   1. Verifica que LLM Studio est√© corriendo")
        print("   2. Verifica la URL en .env")
        print("   3. Verifica que el modelo est√© cargado")
        print("   4. Prueba acceder a la URL desde el navegador")
        print("   5. Verifica que no haya firewall bloqueando el puerto") 