#!/usr/bin/env python3
"""
Script de prueba final con configuraci√≥n corregida de LLM Studio
"""

import asyncio
import os
from crawl4ai import AsyncWebCrawler, LLMConfig
from crawl4ai.extraction_strategy import LLMExtractionStrategy
from dotenv import load_dotenv

# Cargar variables de entorno
load_dotenv()

async def test_llm_inspector_fixed():
    """Prueba el LLM Inspector con configuraci√≥n corregida"""
    print("üß™ PRUEBA: LLM Inspector con configuraci√≥n corregida")
    print("="*60)
    
    # Verificar configuraci√≥n
    llm_studio_url = os.getenv("LLM_STUDIO_BASE_URL")
    llm_studio_model = os.getenv("LLM_STUDIO_MODEL", "gemma-3-12b")
    
    if not llm_studio_url:
        print("‚ùå LLM_STUDIO_BASE_URL no configurado")
        return False
    
    print(f"‚úÖ Configuraci√≥n detectada:")
    print(f"   URL: {llm_studio_url}")
    print(f"   Modelo: {llm_studio_model}")
    
    # ‚úÖ CONFIGURACI√ìN CORREGIDA: Usar la que funciona
    llm_config = LLMConfig(
        provider=f"openai/{llm_studio_model}",
        base_url=llm_studio_url,
        api_token="not-needed"
    )
    
    print(f"üîß Configuraci√≥n LLM creada:")
    print(f"   Provider: {llm_config.provider}")
    print(f"   Base URL: {llm_config.base_url}")
    print(f"   API Token: {llm_config.api_token}")
    
    try:
        async with AsyncWebCrawler(
            verbose=True,
            headless=True,
            always_by_pass_cache=True,
            browser_type="chromium"
        ) as crawler:
            
            print("\nüì° Probando extracci√≥n con LLM Studio...")
            
            result = await crawler.arun(
                url="https://lcd.exactas.uba.ar/materias",
                extraction_strategy=LLMExtractionStrategy(
                    llm_config=llm_config,
                    extraction_type="llm",
                    instruction="What is this webpage about? Answer in one sentence.",
                    chunk_token_threshold=1000,
                    apply_chunking=False
                ),
                bypass_cache=True
            )
            
            print(f"\nüìä Resultado de la prueba:")
            print(f"   Success: {result.success if result else 'No result'}")
            print(f"   Has HTML: {bool(result and result.html)}")
            print(f"   HTML Length: {len(result.html) if result and result.html else 0}")
            print(f"   Has Markdown: {bool(result and result.markdown)}")
            print(f"   Markdown Length: {len(result.markdown) if result and result.markdown else 0}")
            print(f"   Has Extracted Content: {bool(result and hasattr(result, 'extracted_content') and result.extracted_content)}")
            
            if result and hasattr(result, 'extracted_content'):
                print(f"   Extracted Content: '{result.extracted_content}'")
            else:
                print(f"   Extracted Content: None")
            
            if result and hasattr(result, 'error_message') and result.error_message:
                print(f"   Error Message: {result.error_message}")
            
            # Verificar resultado
            if result and result.extracted_content:
                print(f"\nüéâ √âXITO: LLM Studio responde correctamente!")
                print(f"   Contenido extra√≠do: {result.extracted_content[:200]}...")
                return True
            else:
                print(f"\n‚ùå FALLO: LLM Studio no responde")
                return False
                
    except Exception as e:
        print(f"\n‚ùå Error durante la prueba: {e}")
        return False

async def test_schema_extraction():
    """Prueba extracci√≥n con esquema JSON"""
    print("\nüß™ PRUEBA: Extracci√≥n con esquema JSON")
    print("="*60)
    
    llm_studio_url = os.getenv("LLM_STUDIO_BASE_URL")
    llm_studio_model = os.getenv("LLM_STUDIO_MODEL", "gemma-3-12b")
    
    # Configuraci√≥n LLM
    llm_config = LLMConfig(
        provider=f"openai/{llm_studio_model}",
        base_url=llm_studio_url,
        api_token="not-needed"
    )
    
    # Esquema simple para prueba
    schema = {
        "type": "object",
        "properties": {
            "title": {"type": "string"},
            "description": {"type": "string"}
        }
    }
    
    try:
        async with AsyncWebCrawler(
            verbose=True,
            headless=True,
            always_by_pass_cache=True,
            browser_type="chromium"
        ) as crawler:
            
            print("üì° Probando extracci√≥n con esquema...")
            
            result = await crawler.arun(
                url="https://lcd.exactas.uba.ar/materias",
                extraction_strategy=LLMExtractionStrategy(
                    llm_config=llm_config,
                    schema=schema,
                    extraction_type="schema",
                    instruction="Extract the title and description of this webpage."
                ),
                bypass_cache=True
            )
            
            print(f"\nüìä Resultado del esquema:")
            print(f"   Success: {result.success if result else 'No result'}")
            print(f"   Has Extracted Content: {bool(result and hasattr(result, 'extracted_content') and result.extracted_content)}")
            
            if result and hasattr(result, 'extracted_content'):
                print(f"   Extracted Content: '{result.extracted_content}'")
                return True
            else:
                print(f"   Extracted Content: None")
                return False
                
    except Exception as e:
        print(f"\n‚ùå Error en esquema: {e}")
        return False

if __name__ == "__main__":
    print("üöÄ Iniciando prueba final con configuraci√≥n corregida...")
    
    # Verificar archivo .env
    if not os.path.exists('.env'):
        print("‚ùå Error: Archivo .env no encontrado")
        exit(1)
    
    print("\n" + "="*70)
    print("üîç PRUEBA FINAL CON CONFIGURACI√ìN CORREGIDA")
    print("="*70)
    
    # Ejecutar pruebas
    success1 = asyncio.run(test_llm_inspector_fixed())
    success2 = asyncio.run(test_schema_extraction())
    
    # Resumen final
    print("\n" + "="*70)
    print("üìã RESUMEN FINAL")
    print("="*70)
    
    print(f"üí¨ Extracci√≥n LLM simple: {'‚úÖ' if success1 else '‚ùå'}")
    print(f"üìã Extracci√≥n con esquema: {'‚úÖ' if success2 else '‚ùå'}")
    
    if success1 and success2:
        print("\nüéâ ¬°√âXITO TOTAL! LLM Studio funciona correctamente con Crawl4AI")
        print("   El script llm_css_inspector.py ahora deber√≠a funcionar")
    elif success1:
        print("\n‚ö†Ô∏è  Parcial: LLM simple funciona, esquema puede tener problemas")
    else:
        print("\n‚ùå FALLO: Problema persistente con Crawl4AI y LLM Studio") 